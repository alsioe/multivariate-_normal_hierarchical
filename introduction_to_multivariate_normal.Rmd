---
title: "Multivariate normal strategy"
author: "Johan Alsi√∂"
date: "`r Sys.Date()`"
output:
    html_document:
        keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# rm(list = ls())
# install.packages('viridis')

library(tidyverse)
library(viridis)
library(MASS)

```

## Multivariate normal hierarchical models

A number of different data sets will be explored.
1. One population with a normal distribution, different-sized samples

Modelling data.

We will randomise some data for our first example using the rnorm() function,
which draws random samples from a normal distribution with 'location' (mean) and
'scale' (sd = standard deviation). Note that we need many draws to be able to 
observe the normal distribution visually.

```{r normal distribution}
data <- rbind(data.frame(group = 'V', value = rnorm(5, 0, 1)),
              data.frame(group = 'X', value = rnorm(10, 0, 1)),
              data.frame(group = 'L', value = rnorm(50, 0, 1)),
              data.frame(group = 'C', value = rnorm(100, 0, 1))
              )
    
data$group <- factor(x = data$group,
                    levels = c('V', 'X', 'L', 'C')
                    )

# Plot as histograms
ggplot(data = data,
       mapping = aes(x = value)
       ) +
    facet_wrap(~ group,
               scales = 'free') +
    geom_histogram() +
    theme_bw()

# Plotting as densities, with the true probability density in red
ggplot(data = data,
       mapping = aes(x = value)
       ) +
    facet_wrap(~ group,
               scales = 'free') +
    geom_density() +
    geom_function(fun = dnorm,
                  colour = "red",
                  linetype = 'dashed') +
    theme_bw()

```

```{r Stan model for simple, normal distributions}
# We can ask Stan to find the parameters of any or all of those
normal_data_for_stan <- list(n_y = dim(data)[1],
                             n_groups = length(unique(data$group)),
                             group = match(data$group, unique(data$group)),
                             y = data$value)

fit_normal <- rstan::stan(file = 'stan scripts/normal_distribution.stan',
                          model_name = 'normal distribution',
                          data = normal_data_for_stan,
                          chains = 4,
                          cores = 4,
                          iter = 4000,
                          warmup = 2000,
                          control = list(adapt_delta = 0.99)
                          ) 

# Inspect the trace plots - we want them to look like 'hairy caterpillars''
# https://druedin.com/2016/12/26/that-hairy-caterpillar/
rstan::traceplot(fit_normal)

# Create summary object and print out the overall summary (a.k.a. summary$summary)
summary <- rstan::summary(fit_normal,
                     probs = c(0.025, 0.5, 0.975)
                     )

# We want all the Rhats to be less than 1.01, and we need n_eff to be >100 but
# ideally in the thousands
round(summary$summary, 2)

# Extract the posterior distributions from the fit
par <- rstan::extract(fit_normal)

# Save all samples from the posterior into a data frame
df <- data.frame(
    iter = length(par$mu[, 1]),
    V_mu = par$mu[, 1],
    X_mu = par$mu[, 2],
    L_mu = par$mu[, 3],
    C_mu = par$mu[, 4],
    V_sigma = par$sigma[, 1],
    X_sigma = par$sigma[, 2],
    L_sigma = par$sigma[, 3],
    C_sigma = par$sigma[, 4]
    )

# Re-format for easy graphing
df_long <- pivot_longer(data = df,
                        cols = !iter,
                        names_to = 'parameter',
                        values_to = 'sample')

par_names <- c('V_mu', 'X_mu', 'L_mu', 'C_mu',
               'V_sigma', 'X_sigma', 'L_sigma', 'C_sigma')

# Make the parameter column a factor for easier data handling and graphing
df_long$parameter <- factor(x = df_long$parameter,
                            levels = par_names
                            )

# Plot the probability densities with vertical lines (geom_vline) at 'real' value
ggplot(data = df_long,
       mapping = aes(x = sample)
       ) +
    facet_wrap(~ parameter,
               scales = 'free',
               ncol = 4) +
    geom_density() +
    geom_vline(data = data.frame(parameter = factor(par_names, levels = par_names),
                          xintercept = c(0, 0, 0, 0, 1, 1, 1, 1)
                          ),
               aes(xintercept = xintercept),
               colour = 'red',
               linetype = 'dashed') +
    theme_bw()


```

If we sample from two different values with appropriate distributions, we can use
the values to simulate data from a linear relationship, y = alpha + beta*x

```{r many normal distributions}

# samples to draw from each distribution
N <- 10

true_linear <- data.frame(
        subj_id = 1:N,
        alpha = rnorm(N, 10, 2),
        beta = rnorm(N, 1, 0.5)
        )

true_linear_long <- true_linear %>%
    pivot_longer(cols = !subj_id,
                 names_to = 'parameter',
                 values_to = 'value')

true_linear_long$parameter <- factor(x = true_linear_long$parameter,
                              levels = c('alpha', 'beta')
                              )

# Set up a function to get from the parameters to the simulated data
linear <- function(alpha, beta, x) {
    y = alpha + beta * x

    return(y)
}

# Plot the histograms
ggplot(data = true_linear_long,
       mapping = aes(x = value)
       ) +
    facet_wrap(~ parameter,
               scales = 'free') +
    geom_histogram() +
    theme_bw()

# Set up a palette
my_palette <- viridis::viridis(n = N)

# Set up canvas
plot <- ggplot() +
        xlim(-10, 10) +
        theme_bw()

# Add the individual curves    
for (n in 1:N) {
    plot <- plot +
    geom_function(fun = linear,
                  args = list(alpha = true_linear$alpha[n],
                              beta = true_linear$beta[n]
                              ),
                  colour = my_palette[n]
                  )        
}

plot


```

Any true data will also come with measurement noise etc., so we will add a extra layer. 

```{r noisy data from two normal distributions}

# We now know how to draw perfect data - but we want noisy data!
# Let's add some noise and collect data in duplicates at various x values

n_rep <- 2
x_series <- seq(from = -10, to = 10, by = 5)

# Set up a data frame with the data for the linear association
normal_data <- data.frame(
    subj_id = gl(n = N, k = n_rep * length(x_series)),
    rep = rep(x = 1:n_rep,
              times = N * length(x_series)),
    x = rep(x = sort(rep(x_series, n_rep)),
            times = N),
    y = NA
)

# In this (simplistic) case we will have a common noise level on all data
sd_y <- 10

normal_data$y <- linear(alpha = true_linear$alpha[normal_data$subj_id],
                        beta = true_linear$beta[normal_data$subj_id],
                        x = normal_data$x) +
                 rnorm(n = length(normal_data$y),
                       mean = 0,
                       sd = sd_y)

# Example plots (the filter makes sure we only plot up to 4 subjects)
ggplot(data = filter(normal_data, subj_id %in% c(1:4)),
       mapping = aes(x = x,
                     y = y,
                     colour = subj_id)) +
    geom_jitter(width = 0.1) +
    geom_smooth(method = 'lm') +
    theme_bw()

```

```{r stan to fit linear model}

# ... and we can now ask Stan to try to estimate the starting parameters!
# 

# We can ask Stan to find the parameters of any or all of those
linear_data_for_stan <- list(n_y = dim(normal_data)[1],
                             n_subj = length(unique(normal_data$subj_id)),
                             subj_id = as.integer(normal_data$subj_id),
                             x = normal_data$x,
                             y = normal_data$y)

# Note to self: the standard deviations are not informed by the data so should
# be removed from this, the simplest model!

fit_linear <- rstan::stan(file = 'stan scripts/linear.stan',
                          model_name = 'linear',
                          data = linear_data_for_stan,
                          chains = 4,
                          cores = 4,
                          iter = 4000,
                          warmup = 2000,
                          control = list(adapt_delta = 0.99)
                          ) 

# Inspect the trace plots - we want them to look like 'hairy caterpillars''
# https://druedin.com/2016/12/26/that-hairy-caterpillar/
rstan::traceplot(fit_linear)

# Create summary object and print out the overall summary (a.k.a. summary$summary)
summary <- rstan::summary(fit_linear,
                     probs = c(0.025, 0.5, 0.975)
                     )

# We want all the Rhats to be less than 1.01, and we need n_eff to be >100 but
# ideally in the thousands
round(summary$summary, 2)

# Extract the posterior distributions from the fit
par <- rstan::extract(fit_linear)

par_names <- c('alpha_by_subj',
               'beta_by_subj',
               'sd_alpha',
               'sd_beta',
               'sd_y')

for (s in 1:N) {
    if (s == 1) {
        df <- data.frame(
            subj_id = vector(),
            iter = vector(),
            alpha_by_subj = vector(),
            beta_by_subj = vector()
        )
    }
    
    df <- rbind(df,
                data.frame(
                    subj_id = rep(s, length(par$alpha_by_subj[, s])),
                    iter = 1:length(par$alpha_by_subj[, s]),
                    alpha_by_subj = par$alpha_by_subj[, s],
                    beta_by_subj = par$beta_by_subj[, s]
                    )
                )
}

df_long <- pivot_longer(data = df,
                        cols = !c(subj_id, iter),
                        names_to = 'parameter',
                        values_to = 'sample')

df_long <- rbind(df_long,
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'sd_alpha',
                    sample = par$sd_alpha
                    ),
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'sd_beta',
                    sample = par$sd_beta
                    ),
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'sd_y',
                    sample = par$sd_y
                    )
                 )
    
par_names <- unique(df_long$parameter)

# Make the parameter column a factor for easier data handling and graphing
df_long$parameter <- factor(x = df_long$parameter,
                            levels = par_names
                            )

# Plot the probability densities with vertical lines (geom_vline) at 'real' value
ggplot(data = df_long,
       mapping = aes(x = sample,
                     colour = as.factor(subj_id))
       ) +
    facet_wrap(~ parameter,
               scales = 'free',
               ncol = 4) +
    geom_density() +
    theme_bw()

# Quantile function with updated probs
# https://www.tidyverse.org/blog/2023/02/dplyr-1-1-0-pick-reframe-arrange/
quantile_df <- function(x, probs = c(0.025, 0.5, 0.975)) {
  tibble(
    value = quantile(x, probs, na.rm = TRUE),
    prob = probs
  )
}

# ALPHA PARAMETER
# 
# plot(x = true_linear$beta,
#      y = colMeans(par$beta_by_subj)
#      )

df_wide_quantiles <- df_long %>%
                        filter(parameter == 'alpha_by_subj') %>%
                        reframe(quantile_df(sample), .by = subj_id) %>%
                        pivot_wider(id_cols = c(subj_id),
                                    names_from = prob,
                                    values_from = value)

names(df_wide_quantiles) <- c('subj_id', 'low', 'median', 'high')

ggplot(data = df_wide_quantiles,
       mapping = aes(y = median,
                     ymin = low,
                     ymax = high,
                     colour = as.factor(subj_id))
       ) +
    geom_point(aes(x = true_linear$alpha,
                   y = true_linear$alpha)) +
    geom_errorbar(aes(x = true_linear$alpha),
                  width=.2) +
    theme_bw()

# BETA PARAMETER
df_wide_quantiles <- df_long %>%
                        filter(parameter == 'beta_by_subj') %>%
                        reframe(quantile_df(sample), .by = subj_id) %>%
                        pivot_wider(id_cols = c(subj_id),
                                    names_from = prob,
                                    values_from = value)

names(df_wide_quantiles) <- c('subj_id', 'low', 'median', 'high')

ggplot(data = df_wide_quantiles,
       mapping = aes(y = median,
                     ymin = low,
                     ymax = high,
                     colour = as.factor(subj_id))
       ) +
    geom_point(aes(x = true_linear$beta,
                   y = true_linear$beta)) +
    geom_errorbar(aes(x = true_linear$beta),
                  width=.1) +
    theme_bw()

```

The estimates are very wide at this point, representing the fact that the measurement
noise (sd_y) is quite high, and we only have two measurements per x value. If we were
able to re-run the experiment, perhaps we would improve the measurements - or simply
try to collect more measurements per x value! But if we cannot easily re-do the
experiment, we can still improve the fit by making some additions to the model.

i) Assume that the individuals represent draws from a normally distributed population.
ii) Assume that the parameters come from a multivariate normal distribution

```{r introducing hierarchical modelling}

# The updated stan file is using hierarchical priors on the subjects' parameters.
# alpha_by_subj ~ normal(alpha_mean, sd_alpha;
hierarchical_linear_fit <- rstan::stan(file = 'stan scripts/centered_hierarchical_linear.stan',
                                       model_name = 'hierarchical_linear',
                                       data = linear_data_for_stan,
                                       chains = 4,
                                       cores = 4,
                                       iter = 4000,
                                       warmup = 2000,
                                       control = list(adapt_delta = 0.99)
                                       ) 

# There are a number of error messages about divergent transitions - this is standard
# when running not-yet optimised hierarchical models and it is not a major concern
# until they start to ramp up in numbers in more complex models. We can sort them
# straight away though - these divergent transitions are caused by the
# correlations between the parameters-by-subj and the group mean.
# A simple trick is to used the 'non-centered parameterisation' which
# treats each subject's value as the sum of the mean and some 'raw' subject effect,
# which is the difference between the subject's value and the mean, measured in
# standard deviations.
# 
# https://mc-stan.org/docs/stan-users-guide/reparameterization.html
# 
# alpha_by_subj = alpha_mean + alpha_by_subj_raw * sd_alpha;
# We then put a 'standard normal' prior on the raw subject value
# alpha_by_subj_raw ~ normal(0, 3);
# (Note that the function std_normal() is identifical to normal(0, 3))

hierarchical_linear_fit <- rstan::stan(file = 'stan scripts/hierarchical_linear.stan',
                                       model_name = 'hierarchical_linear',
                                       data = linear_data_for_stan,
                                       chains = 4,
                                       cores = 4,
                                       iter = 4000,
                                       warmup = 2000,
                                       control = list(adapt_delta = 0.99)
                                       )

# Hopefully no divergent transitions! The alpha_by_subj_raw does NOT interact
# in the same way with the overall mean, and the sampler runs smoothly.

# Inspect the trace plots - we want them to look like 'hairy caterpillars''
# https://druedin.com/2016/12/26/that-hairy-caterpillar/
rstan::traceplot(hierarchical_linear_fit)

# Create summary object and print out the overall summary (a.k.a. summary$summary)
summary <- rstan::summary(hierarchical_linear_fit,
                     probs = c(0.025, 0.5, 0.975)
                     )

# We want all the Rhats to be less than 1.01, and we need n_eff to be >100 but
# ideally in the thousands
round(summary$summary, 2)

# Extract the posterior distributions from the fit
par <- rstan::extract(hierarchical_linear_fit)

par_names <- c('alpha_mean',
               'beta_mean',
               'alpha_by_subj',
               'beta_by_subj',
               'sd_alpha',
               'sd_beta',
               'sd_y')

for (s in 1:N) {
    if (s == 1) {
        df <- data.frame(
            subj_id = vector(),
            iter = vector(),
            alpha_by_subj = vector(),
            beta_by_subj = vector()
        )
    }
    
    df <- rbind(df,
                data.frame(
                    subj_id = rep(s, length(par$alpha_by_subj[, s])),
                    iter = 1:length(par$alpha_by_subj[, s]),
                    alpha_by_subj = par$alpha_by_subj[, s],
                    beta_by_subj = par$beta_by_subj[, s]
                    )
                )
}

df_long <- pivot_longer(data = df,
                        cols = !c(subj_id, iter),
                        names_to = 'parameter',
                        values_to = 'sample')

df_long <- rbind(df_long,
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'alpha_mean',
                    sample = par$alpha_mean
                    ),
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'beta_mean',
                    sample = par$beta_mean
                    ),
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'sd_alpha',
                    sample = par$sd_alpha
                    ),
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'sd_beta',
                    sample = par$sd_beta
                    ),
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'sd_y',
                    sample = par$sd_y
                    )
                 )
    
par_names <- unique(df_long$parameter)

# Make the parameter column a factor for easier data handling and graphing
df_long$parameter <- factor(x = df_long$parameter,
                            levels = par_names
                            )

# Plot the probability densities with vertical lines (geom_vline) at 'real' value
ggplot(data = df_long,
       mapping = aes(x = sample,
                     colour = as.factor(subj_id))
       ) +
    facet_wrap(~ parameter,
               scales = 'free',
               ncol = 4) +
    geom_density() +
    theme_bw()

# Quantile function with updated probs
# https://www.tidyverse.org/blog/2023/02/dplyr-1-1-0-pick-reframe-arrange/
quantile_df <- function(x, probs = c(0.025, 0.5, 0.975)) {
  tibble(
    value = quantile(x, probs, na.rm = TRUE),
    prob = probs
  )
}

# ALPHA PARAMETER
# 
# plot(x = true_linear$beta,
#      y = colMeans(par$beta_by_subj)
#      )

df_wide_quantiles <- df_long %>%
                        filter(parameter == 'alpha_by_subj') %>%
                        reframe(quantile_df(sample), .by = subj_id) %>%
                        pivot_wider(id_cols = c(subj_id),
                                    names_from = prob,
                                    values_from = value)

names(df_wide_quantiles) <- c('subj_id', 'low', 'median', 'high')

ggplot(data = df_wide_quantiles,
       mapping = aes(y = median,
                     ymin = low,
                     ymax = high,
                     colour = as.factor(subj_id))
       ) +
    geom_point(aes(x = true_linear$alpha,
                   y = true_linear$alpha)) +
    geom_errorbar(aes(x = true_linear$alpha),
                  width=.2) +
    theme_bw()

# BETA PARAMETER
df_wide_quantiles <- df_long %>%
                        filter(parameter == 'beta_by_subj') %>%
                        reframe(quantile_df(sample), .by = subj_id) %>%
                        pivot_wider(id_cols = c(subj_id),
                                    names_from = prob,
                                    values_from = value)

names(df_wide_quantiles) <- c('subj_id', 'low', 'median', 'high')

ggplot(data = df_wide_quantiles,
       mapping = aes(y = median,
                     ymin = low,
                     ymax = high,
                     colour = as.factor(subj_id))
       ) +
    geom_point(aes(x = true_linear$beta,
                   y = true_linear$beta)) +
    geom_errorbar(aes(x = true_linear$beta),
                  width=.1) +
    theme_bw()

```

## Introduce correlations between parameters

In the real world, parameters can often be correlated. Imagine for instance
that subjects with a high intercept have a lower slope parameter.
We can simulate this using the function mvrnorm() from the MASS package in R.
This function creates a multivariate normal distribution,
which is simply a set of k parameters, each of which is normally distributed
and which are specified to share some covariance (correlations).

```{r multinormal}

# The function mvrnorm() requires k means but and we then need to supply a
# k x k covariance matrix Sigma, which contains information both about the
# standard deviations sigma, a vector of length k, and the correlations between
# the parameters, which can be described as a k xk correlation matrix Omega. 
# It is usually easier to specify the correlation matrix Omega,
# and simply transform it to the covariance matrix Sigma, like so:

# Update any settings?
N <- 20
n_rep <- 2
x_series <- seq(from = -10, to = 10, by = 5)
sd_y <- 5

# Create data structure
normal_data$y <- linear(alpha = true_linear$alpha[normal_data$subj_id],
                        beta = true_linear$beta[normal_data$subj_id],
                        x = normal_data$x) +
                 rnorm(n = length(normal_data$y),
                       mean = 0,
                       sd = sd_y)

Omega <- matrix(data = c(1, -0.9,
                         -0.9, 1),
                ncol = 2)

mu <- c(10, 3)
sigma <- c(10, 3)
 
# Transform from correlation matrix to covariance matrix
Sigma <- diag(sigma) %*% Omega %*% diag(sigma)
 
true_multi_normal <- data.frame(
                    subj_id = 1:N,
                    alpha = NA,
                    beta = NA
                    )

true_multi_normal[, 2:3] <- MASS::mvrnorm(n = N,
                                              mu = mu,
                                              Sigma = Sigma)

plot(x = true_multi_normal$alpha,
     y = true_multi_normal$beta)

cor(x = true_multi_normal$alpha,
    y = true_multi_normal$beta)

# We can then create a new data structure ...
multi_normal_data <- data.frame(
    subj_id = gl(n = N, k = n_rep * length(x_series)),
    rep = rep(x = 1:n_rep,
              times = N * length(x_series)),
    x = rep(x = sort(rep(x_series, n_rep)),
            times = N),
    y = NA
)

# And randomise some values (with noise)
multi_normal_data$y <-
    linear(
        alpha = true_multi_normal$alpha[multi_normal_data$subj_id],
        beta = true_multi_normal$beta[multi_normal_data$subj_id],
        x = normal_data$x
        ) +
    rnorm(n = length(multi_normal_data$y),
        mean = 0,
        sd = sd_y)
# 
# Example plots - note that slopes get lower, the higher the intercept!
ggplot(data = filter(multi_normal_data, subj_id %in% c(1, 2, 3, 4, 5, 6)),
       mapping = aes(x = x,
                     y = y,
                     colour = subj_id)) +
    geom_jitter(width = 0.1) +
    geom_smooth(method = 'lm') +
    theme_bw()

```

What happens if we try to model this in Stan?

```{r noisy data from normal distributions}

multi_normal_data_for_stan <- list(n_y = dim(multi_normal_data)[1],
                             n_subj = length(unique(multi_normal_data$subj_id)),
                             n_theta = 2,
                             subj_id = as.integer(multi_normal_data$subj_id),
                             x = multi_normal_data$x,
                             y = multi_normal_data$y)

# The model itself has got some unexpected matrix structures - those are there
# to handle the effects of within-subject conditions and between-subject effects
# - they will allow us to seamlessly move to mixed within-between effects
multi_normal_linear_fit <- rstan::stan(file = 'stan scripts/multi_normal_linear.stan',
                                       model_name = 'multi_normal_linear',
                                       data = multi_normal_data_for_stan,
                                       chains = 4,
                                       cores = 4,
                                       iter = 4000,
                                       warmup = 2000,
                                       control = list(adapt_delta = 0.99)
                                       )

# For comparison, here is the hierarchical model without the multi_normal structure
hierarchical_linear_fit <- rstan::stan(file = 'stan scripts/hierarchical_linear.stan',
                                       model_name = 'hierarchical_linear',
                                       data = multi_normal_data_for_stan,
                                       chains = 4,
                                       cores = 4,
                                       iter = 4000,
                                       warmup = 2000,
                                       control = list(adapt_delta = 0.99)
                                       )

# Inspect the trace plots - we want them to look like 'hairy caterpillars''
# https://druedin.com/2016/12/26/that-hairy-caterpillar/
rstan::traceplot(multi_normal_linear_fit)

# Create summary object and print out the overall summary (a.k.a. summary$summary)
summary_multi <- rstan::summary(multi_normal_linear_fit,
                          par = c('alpha_by_subj',
                                  'beta_by_subj'),
                          probs = c(0.025, 0.5, 0.975)
                          )

summary_hierarchical <- rstan::summary(hierarchical_linear_fit,
                          par = c('alpha_by_subj',
                                  'beta_by_subj'),
                          probs = c(0.025, 0.5, 0.975)
                          )

par_multi <- rstan::extract(multi_normal_linear_fit,
                            par = c('alpha_mean',
                                    'beta_mean',
                                    'alpha_by_subj',
                                    'beta_by_subj',
                                    'sd_alpha',
                                    'sd_beta',
                                    'sd_y')
                            )

par_hierarchical <- rstan::extract(hierarchical_linear_fit,
                                   par = c('alpha_mean',
                                           'beta_mean',
                                           'alpha_by_subj',
                                           'beta_by_subj',
                                           'sd_alpha',
                                           'sd_beta',
                                           'sd_y')
                                   )

par(mfrow = c(3, 3))

plot(x = colMeans(par_hierarchical$alpha_by_subj),
     y = colMeans(par_multi$alpha_by_subj[, 1, ])
     )

plot(x = colMeans(par_hierarchical$beta_by_subj),
     y = colMeans(par_multi$beta_by_subj[, 1, ])
     )

plot(x = cor(colMeans(par_hierarchical$alpha_by_subj), colMeans(par_hierarchical$beta_by_subj)),
     y = cor(colMeans(par_multi$alpha_by_subj[, 1, ]), colMeans(par_multi$beta_by_subj[, 1 ,]))
     )

plot(x = true_multi_normal$alpha,
     y = colMeans(par_multi$alpha_by_subj[, 1, ])
     )

plot(x = true_multi_normal$beta,
     y = colMeans(par_multi$beta_by_subj[, 1, ]))

plot(x = colMeans(par_multi$alpha_by_subj),
     y = colMeans(par_multi$beta_by_subj)
     )

plot(x = true_multi_normal$alpha,
     y = colMeans(par_hierarchical$alpha_by_subj)
     )

plot(x = true_multi_normal$beta,
     y = colMeans(par_hierarchical$beta_by_subj)
     )

plot(x = colMeans(par_hierarchical$alpha_by_subj),
     y = colMeans(par_hierarchical$beta_by_subj)
     )

par(mfrow = c(1, 1))

```

Multivariate normal - a closer look

```{r a closer look into the results from the multi_normal}

# Extract the posterior distributions from the fit
par <- rstan::extract(multi_normal_linear_fit,
                      par = c('alpha_mean',
                              'beta_mean',
                              'alpha_by_subj',
                              'beta_by_subj',
                              'sd_alpha',
                              'sd_beta',
                              'sd_y')
                      )

par_names <- c('alpha_mean',
               'beta_mean',
               'alpha_by_subj',
               'beta_by_subj',
               'sd_alpha',
               'sd_beta',
               'sd_y')

for (s in 1:N) {
    if (s == 1) {
        df <- data.frame(
            subj_id = vector(),
            iter = vector(),
            alpha_by_subj = vector(),
            beta_by_subj = vector()
        )
    }
    
    df <- rbind(df,
                data.frame(
                    subj_id = rep(s, length(par$alpha_by_subj[, 1, s])),
                    iter = 1:length(par$alpha_by_subj[, 1, s]),
                    alpha_by_subj = par$alpha_by_subj[, 1, s],
                    beta_by_subj = par$beta_by_subj[, 1, s]
                    )
                )
}

df_long <- pivot_longer(data = df,
                        cols = !c(subj_id, iter),
                        names_to = 'parameter',
                        values_to = 'sample')

df_long <- rbind(df_long,
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'alpha_mean',
                    sample = par$alpha_mean
                    ),
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'beta_mean',
                    sample = par$beta_mean
                    ),
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'sd_alpha',
                    sample = par$sd_alpha
                    ),
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'sd_beta',
                    sample = par$sd_beta
                    ),
                 data.frame(
                    subj_id = rep(0, length(par$sd_y)),
                    iter = 1:length(length(par$sd_y)),
                    parameter = 'sd_y',
                    sample = par$sd_y
                    )
                 )
    
par_names <- unique(df_long$parameter)

# Make the parameter column a factor for easier data handling and graphing
df_long$parameter <- factor(x = df_long$parameter,
                            levels = par_names
                            )

# Plot the probability densities with vertical lines (geom_vline) at 'real' value
ggplot(data = df_long,
       mapping = aes(x = sample,
                     colour = as.factor(subj_id))
       ) +
    facet_wrap(~ parameter,
               scales = 'free',
               ncol = 4) +
    geom_density() +
    theme_bw()

# Quantile function with updated probs
# https://www.tidyverse.org/blog/2023/02/dplyr-1-1-0-pick-reframe-arrange/
quantile_df <- function(x, probs = c(0.025, 0.5, 0.975)) {
  tibble(
    value = quantile(x, probs, na.rm = TRUE),
    prob = probs
  )
}

df_alpha_quantiles <- df_long %>%
                        filter(parameter == 'alpha_by_subj') %>%
                        reframe(quantile_df(sample), .by = subj_id) %>%
                        pivot_wider(id_cols = c(subj_id),
                                    names_from = prob,
                                    values_from = value)

names(df_alpha_quantiles) <- c('subj_id', 'low', 'median', 'high')

ggplot(data = df_alpha_quantiles,
       mapping = aes(y = median,
                     ymin = low,
                     ymax = high,
                     colour = as.factor(subj_id))
       ) +
    geom_point(aes(x = true_multi_normal$alpha,
                   y = true_multi_normal$alpha)) +
    geom_errorbar(aes(x = true_multi_normal$alpha),
                  width=.2) +
    theme_bw()

# BETA PARAMETER
df_beta_quantiles <- df_long %>%
                        filter(parameter == 'beta_by_subj') %>%
                        reframe(quantile_df(sample), .by = subj_id) %>%
                        pivot_wider(id_cols = c(subj_id),
                                    names_from = prob,
                                    values_from = value)

names(df_beta_quantiles) <- c('subj_id', 'low', 'median', 'high')

ggplot(data = df_beta_quantiles,
       mapping = aes(y = median,
                     ymin = low,
                     ymax = high,
                     colour = as.factor(subj_id))
       ) +
    geom_point(aes(x = true_multi_normal$beta,
                   y = true_multi_normal$beta)) +
    geom_errorbar(aes(x = true_multi_normal$beta),
                  width=.1) +
    theme_bw()

# CORRELATION!
ggplot(data = df_alpha_quantiles,
       mapping = aes(y = median,
                     ymin = low,
                     ymax = high,
                     colour = as.factor(subj_id))
       ) +
    geom_point(aes(x = df_beta_quantiles$median)) +
    theme_bw()


cor(df_alpha_quantiles$median,
    df_beta_quantiles$median)

estimated_Omega <- rstan::extract(multi_normal_linear_fit, par = c('Sigma', 'L', 'Omega'))

colMeans(estimated_Omega$Sigma)
colMeans(estimated_Omega$L)
colMeans(estimated_Omega$Omega)


```

Sigmoid
```{r sigmoid 4PL}
# Update any settings?

N <- 20
n_rep <- 3
x_series <- seq(from = -10, to = 10, by = 2)
sd_y <- 5

Omega <- matrix(data = c(   1, -0.9,  0.2,    0,
                            -0.9,    1,  0.1,    0,
                            0.2,  0.1,    1, -0.5,
                            0,    0, -0.5,    1),
                ncol = 4)

mu <- c(95, 1, 5, 10)
sigma <- c(10, 0.2, 1, 3)

# Transform from correlation matrix to covariance matrix
Sigma <- diag(sigma) %*% Omega %*% diag(sigma)

true_multi_normal <- data.frame(
    subj_id = 1:N,
    alpha = NA,
    beta = NA,
    gamma = NA,
    delta = NA
)

true_multi_normal[, 2:5] <- MASS::mvrnorm(n = N,
                                          mu = mu,
                                          Sigma = Sigma)

# We can then create a new data structure ...
multi_normal_data <- data.frame(
    subj_id = gl(n = N, k = n_rep * length(x_series)),
    rep = rep(x = 1:n_rep,
              times = N * length(x_series)),
    x = rep(x = sort(rep(x_series, n_rep)),
            times = N),
    y = NA
)

logistic_4PL <- function(alpha, beta, gamma, delta, x) {
    y <- delta + (alpha - delta) / (1 + exp(-beta * (x - gamma)))
    return(y)
}

#Test the function
    logistic_4PL(
        alpha = true_multi_normal$alpha[multi_normal_data$subj_id],
        beta = true_multi_normal$beta[multi_normal_data$subj_id],
        gamma = true_multi_normal$gamma[multi_normal_data$subj_id],
        delta = true_multi_normal$delta[multi_normal_data$subj_id],
        x = multi_normal_data$x
    )

# And randomise some values (with noise)
multi_normal_data$y <-
    logistic_4PL(
        alpha = true_multi_normal$alpha[multi_normal_data$subj_id],
        beta = true_multi_normal$beta[multi_normal_data$subj_id],
        gamma = true_multi_normal$gamma[multi_normal_data$subj_id],
        delta = true_multi_normal$delta[multi_normal_data$subj_id],
        x = multi_normal_data$x
    ) +
    # Add random noise defined by sd_y
    rnorm(n = length(multi_normal_data$y),
          mean = 0,
          sd = sd_y)

multi_normal_data %>%
    group_by(subj_id, x) %>%
    summarise(mean = mean(y))

# Example plots - note that slopes get lower, the higher the intercept!
ggplot(data = filter(multi_normal_data, subj_id %in% c(1, 2, 3, 4, 5, 6)),
       mapping = aes(x = x,
                     y = y,
                     colour = subj_id)) +
    geom_jitter(width = 0.1) +
    theme_bw()

```

Sigmoid in Stan

```{r stan for sigmoid}

multi_normal_data_for_stan <- list(n_y = dim(multi_normal_data)[1],
                                   n_subj = length(unique(multi_normal_data$subj_id)),
                                   n_theta = 4,
                                   subj_id = as.integer(multi_normal_data$subj_id),
                                   x = multi_normal_data$x,
                                   y = multi_normal_data$y)

# Disclaimer - the next model will make a very poor fit. We will learn something
# from the experience by looking at the fit afterwards but the other downside
# is that the samples takes a long time to complete.

# run_stan function
# This script checks if the fit is already saved, before going ahead and
# running the script. If the script is already there, the function will simple
# read it from file instead of going through the trouble of sampling. If the
# fit is not there, the sampler will run and also save the .RDS file.
run_stan <- function(file,
                     data,
                     model_name = 'model',
                     chains = 4,
                     cores = 4,
                     iter = 4000,
                     warmup = 2000,
                     control = list(adapt_delta = 0.99,
                                    max_treedepth = 20),
                     force_run = FALSE,
                     shiny = FALSE) {
    
    fit_name <- paste0('fit_', model_name)
    fit_name_rds <- paste0(fit_name, '.RDS') 
    
    if ( (fit_name_rds %in% list.files('fits/')) & (force_run == FALSE) ) {
        
        # If a file with this name exists and if force_run == FALSE,
        # read the file instead of re-running the sampler
        fit <- readRDS(file = paste0('fits/',
                                     fit_name_rds)
                            )
    
    } else if ( (!fit_name_rds %in% list.files('fits/')) | (force_run == TRUE) ) {

        # If the file does not exist or if force_run == TRUE, sample away!
        fit <- rstan::stan(
            file = file,
            data = data,
            model_name = 'multi_normal_4PL',
            chains = 4,
            cores = 4,
            iter = 4000,
            warmup = 2000,
            control = list(adapt_delta = 0.99,
                           max_treedepth = 20)
            )
        
        saveRDS(object = fit_name,
                file = paste0('fits/',
                              fit_name_rds)
                )
        
    }
    
    return(fit)
    
}

# First attempt - no boundaries set! A 4-parameter logistic function without
# boundaries can "flip" and there are, therefore, many possibly solutions!
fit_multi_normal_4PL_no_boundaries <-
    run_stan(file = 'stan scripts/multi_normal_4PL_no_boundaries.stan',
             data = multi_normal_data_for_stan,
             model_name = 'multi_normal_4PL_no_boundaries')

# Second strategy - with boundaries on beta
fit_multi_normal_4PL_with_boundaries <-
    run_stan(file = 'stan scripts/multi_normal_non-transformed_4PL.stan',
             data = multi_normal_data_for_stan,
             model_name = 'multi_normal_4PL_with_boundaries')

# Third strategy - with transformed values
fit_multi_normal_4PL <-
    run_stan(file = 'stan scripts/multi_normal_4PL.stan',
             data = multi_normal_data_for_stan,
             model_name = 'multi_normal_4PL')

# Best ever example ever of non-convergence
rstan::traceplot(fit_multi_normal_4PL_no_boundaries, par = 'alpha_mean')
rstan::traceplot(fit_multi_normal_4PL_with_boundaries, par = 'alpha_mean')
rstan::traceplot(fit_multi_normal_4PL, par = 'alpha_mean')

par <- rstan::extract(fit_multi_normal_4PL)

par(mfrow = c(2, 2))
plot(x = true_multi_normal$alpha,
     y = colMeans(par$alpha_by_subj))

plot(x = true_multi_normal$beta,
     y = colMeans(par$beta_by_subj))

plot(x = true_multi_normal$gamma,
     y = colMeans(par$gamma_by_subj))

plot(x = true_multi_normal$delta,
     y = colMeans(par$delta_by_subj))
par(mfrow = c(1, 1))

colMeans(par$Omega)
cor(colMeans(par$alpha_by_subj[, 1, ]),
    colMeans(par$beta_by_subj[, 1, ])
    )

```

## Within-subject structure

## Between-subject structure

## Reinforcement learning





## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
